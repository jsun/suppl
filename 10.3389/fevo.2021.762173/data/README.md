# Datasets

This document describes about data preprocessing of odonata images.
There are four original datasets listed on the following table prepared by authors.


| dataset                       | description   |
|-------------------------------|---------------------|
| Tombo-revise-Specimen         | Images were obtained by scanning of specimens. Background of these images are white. |
| Tombo-revise-ZukanPhoto       | Images were obtained by scanning of photo book and web scraping. Background of these images are white. |
| Tombo-revise-CitizenSurvey    | Images were obatined by citizen surveys. Most images were taken by smart-phone or digital cameras in the fields. |
| Tombo-revise-WebScrapingFiled | Images were obtained by web scraping. Background of these images are various. |


The original datasets prepared by authors contained the species ID.
Since ID are not required for model construction, we remove the ID from the sub-directory names.

```
for drpath in Tombo-revise-Specimen Tombo-revise-ZukanPhoto Tombo-revise-CitizenSurvey Tombo-revise-WebScrapingField
do
    echo ${drpath}
    cd ${drpath}
    for dpath in `ls`
    do
        mv ${dpath} ${dpath:4}
    done
    mv 1_Rhyothemis_regia Rhyothemis_regia
    cd ..
done

```

The preprocessings, explain in the following sections, will be perfromed against the four orignal datasets.
The preprocessed images will be sorted as W1, W2, F, W1F, W2F, and T as the following table.


| dataset        | background       | memo                              |
|----------------|------------------|-----------------------------------|
| W1             | white            | Specimen and ZukanPhoto.          |
| W2             | natural          | W1 + background + frontground.    |
| F              | natural          | WebScrapingField.                 |
| W1F            | white/natural    | augmentated-F and augmentated-W1. |
| W2F            | natural          | augmentated-F and augmentated-W2. |
| T              | natural          | validation dataset from CintizenSurvey. |
| Tw             | white            | validation dataset white background. |


## Image Datasets


```bash
PROJECT_PATH=${HOME}/projects/dragonfly
DATA_PATH=${PROJECT_PATH}/data
SCRIPT_PATH=${PROJECT_PATH}/scripts
cd ${DATA_PATH}
```

### Original Dataset

Copy from original dataset.

```bash
mkdir -p dataset_background
mkdir -p dataset_T/raw
mkdir -p dataset_Tw/raw
mkdir -p dataset_F/raw
mkdir -p dataset_W1/raw
mkdir -p dataset_W2/raw
mkdir -p dataset_I/raw

cp -r unsorted/Yamanaka.Fingers.Background2/* dataset_background/
cd dataset_background
mv Background.Sample field
mv Finger.Sample finger
mv Palm.Sample palm
cd ..

cp -r unsorted/Tombo-revise-CitizenSurvey/*       dataset_T/raw/
cp -r unsorted/Tombo-revise-Test100/*             dataset_Tw/raw/
cp -r unsorted/Tombo-revise-WebScrapingField/*    dataset_F/raw/
cp -r unsorted/Tombo-revise-Specimen/*            dataset_W1/raw/
cp -r unsorted/Tombo-revise-ZukanPhoto/*          dataset_W1/raw/

cp -r unsorted/IncidenceRecords/* dataset_I/raw/
```



### Class Labels

The order of labels were manually generated by authors according to the taxonomy.
However, the class label also can be generated automatically according to alphabetical
order with the following code.

```bash
ls -f1 dataset_*/* | \
     grep -v "^$" | grep -v "/" | grep -v "\." | grep -v "izu" | \
     sort | uniq  > dragonfly_classes.txt
```

There are 204 classes in this study.

```bash
wc dragonfly_classes.txt
# 204  204 4362 ../dragonfly_classes.txt
```




### Preprocessing of Image Datasets

#### Dataset T

```bash
dpath=dataset_T
cd ${dpath}

python ${SCRIPT_PATH}/crop_images.py exif raw 

cp -r raw cropped_image
ls cropped_image/*/* | grep -v __cropped__ | xargs rm
rmdir cropped_image/*
rm raw/*/*__cropped__*

cd ..
```

#### Dataset F

```bash
dpath=dataset_F
cd ${dpath}
cd raw

# remove blanks and brackets in file name
for d in `ls`
do
    cd ${d}
    echo ${d}
    for f in *
    do
        python ${SCRIPT_PATH}/rename.py "${f}"
    done
    cd ..
done
cd ..

# crop images
python ${SCRIPT_PATH}/crop_images.py noexif raw

cp -r raw cropped_image
ls cropped_image/*/* | grep -v __cropped__ | xargs rm
rmdir cropped_image/*
rm raw/*/*__cropped__*

# resize images / some images are too big to perform augmentation
cd cropped_image
for d in `ls`; do
    cd ${d}
    for f in `ls`; do
        resized_image=resized_${f}
        python ${SCRIPT_PATH}/resize_images.py ${f} ${resized_image}
        resized_signal=${?}
        echo "${d}/${f}    ${resized_signal}"
        if [ ${resized_signal} -eq 1 ]; then
            rm ${f}
        fi
    done
    cd -
done

cd ..
python ${SCRIPT_PATH}/augmentation.py cropped_image augmentated_image augment
```



#### Dataset W1

```bash
dpath=dataset_W1
cd ${dpath}

# remove blanks and brackets in file name
cd raw
for d in `ls`
do
    cd ${d}
    echo ${d}
    for f in *
    do
        python ${SCRIPT_PATH}/rename.py "${f}"
    done
    cd ..
done

python ${SCRIPT_PATH}/augmentation.py raw augmentated_image augment
```


#### Dataset W2

```bash
dpath=dataset_W2
cd ${dpath}
mkdir -p mask
mkdir -p synthesis
cp -r ../dataset_W1/raw .

python ${SCRIPT_PATH}/make_dragonfly_mask.py raw mask
BACKGROUND_PATH=${DATA_PATH}/dataset_background
python ${SCRIPT_PATH}/make_dragonfly_synthesis.py mask ${BACKGROUND_PATH} synthesis
```



#### Dataset W1F/W2F


Merged data to generate W1F and W2F.

```bash
dpath=dataset_W1F
mkdir ${dpath}
cd ${dpath}
mkdir -p merged

python ${SCRIPT_PATH}/randomcopy_image.py ../dataset_W1/augmentated_image merged 1
python ${SCRIPT_PATH}/randomcopy_image.py ../dataset_F/augmentated_image merged 2

cd ..

dpath=dataset_W2F
mkdir ${dpath}
cd ${dpath}
mkdir -p merged

python ${SCRIPT_PATH}/randomcopy_image.py ../dataset_W2/synthesis merged 1
python ${SCRIPT_PATH}/randomcopy_image.py ../dataset_F/augmentated_image merged 2
```


### Resize


```
cd ${DATA_PATH}

python ${SCRIPT_PATH}/resize224x224.py dataset_F/augmentated_image  dataset_F/train_images
python ${SCRIPT_PATH}/resize224x224.py dataset_W1/augmentated_image dataset_W1/train_images
python ${SCRIPT_PATH}/resize224x224.py dataset_W2/synthesis         dataset_W2/train_images
python ${SCRIPT_PATH}/resize224x224.py dataset_W1F/merged           dataset_W1F/train_images
python ${SCRIPT_PATH}/resize224x224.py dataset_W2F/merged           dataset_W2F/train_images
```




### Dataset ALL

Merge all dataset to train model.

```bash
cd dataset_T
python  ${SCRIPT_PATH}/augmentation.py cropped_image train_images augment


mkdir -p dataset_ALL/train_images

for ds in dataset_F dataset_W1 dataset_W2 dataset_T
do
    for sp in `ls ${ds}/train_images`
    do
        echo ${ds}/train_images/${sp}
        mkdir -p dataset_ALL/train_images/${sp}
        for im in `ls ${ds}/train_images/${sp}`
        do
            cp ${ds}/train_images/${sp}/${im} dataset_ALL/train_images/${sp}/${ds}__${im}
        done
    done
done
```



### Datasets (Genus)

Note that, if file name contains a space, the following script will be failure.
You need manually execute the following process.

```bash
cd ${DATA_PATH}

cut -d'_' -f1 dragonfly_classes.txt | sort | uniq > dragonflyg_classes.txt

for dspath in dataset_F dataset_W1 dataset_W2 dataset_W1F dataset_W2F
do
    mkdir -p ${dspath}g/train_images
    cp -r ${dspath}/train_images/* ${dspath}g/train_images/
    cd ${dspath}g
    for dpath in `ls`
    do
        cd ${dpath}
        for spname in `ls | grep _`
        do
            echo ${spname}
            spnamearr=(${spname//_/ })
            gnname=${spnamearr[0]}
            mkdir -p ${gnname}
            for fpath in `ls ${spname}`
            do
                mv ${spname}/${fpath} ${gnname}/${spname}__${fpath}
            done
            rmdir ${spname}
        done
        cd ..
    done
    cd ..
done


for dspath in dataset_T dataset_Tw
do
    cp -r ${dspath} ${dspath}g
    cd ${dspath}g
    # check all types of datasets
    for dpath in `ls`
    do
        cd ${dpath}
        for spname in `ls | grep _`
        do
            echo ${spname}
            spnamearr=(${spname//_/ })
            gnname=${spnamearr[0]}
            mkdir -p ${gnname}
            for fpath in `ls ${spname}`
            do
                mv ${spname}/${fpath} ${gnname}/${spname}__${fpath}
            done
            rmdir ${spname}
        done
        cd ..
    done
    cd ..
done

```



### Datasets Summary

```bash
cd ${DATA_PATH}
mkdir data_summary

python ../scripts/count_images_in_dirs.py dragonfly_classes.txt dataset_T/cropped_image > data_summary/dataset_T.cropped.tsv
python ../scripts/count_images_in_dirs.py dragonfly_classes.txt dataset_T/raw           > data_summary/dataset_T.raw.tsv
python ../scripts/count_images_in_dirs.py dragonfly_classes.txt dataset_F/cropped_image > data_summary/dataset_F.cropped.tsv
python ../scripts/count_images_in_dirs.py dragonfly_classes.txt dataset_F/raw           > data_summary/dataset_F.raw.tsv
python ../scripts/count_images_in_dirs.py dragonfly_classes.txt dataset_W1/raw          > data_summary/dataset_W1.raw.tsv

python ../scripts/count_images_in_dirs.py dragonflyg_classes.txt dataset_Tg/cropped_image > data_summary/dataset_Tg.cropped.tsv
python ../scripts/count_images_in_dirs.py dragonflyg_classes.txt dataset_Tg/raw           > data_summary/dataset_Tg.raw.tsv
python ../scripts/count_images_in_dirs.py dragonflyg_classes.txt dataset_Fg/cropped_image > data_summary/dataset_Fg.cropped.tsv
python ../scripts/count_images_in_dirs.py dragonflyg_classes.txt dataset_Fg/raw           > data_summary/dataset_Fg.raw.tsv
python ../scripts/count_images_in_dirs.py dragonflyg_classes.txt dataset_W1g/raw          > data_summary/dataset_W1g.raw.tsv

python ../scripts/print_image_capure_dates.py dataset_T/raw > data_summary/dataset_T.exif.tsv
```

```r
# !cd data_summary

for (tag in c('raw', 'cropped')) {
    for (sp in c('', 'g')) {
        if (tag != 'cropped') {
            w1 <- read.table(paste0('dataset_W1', sp, '.', tag, '.tsv'), header = FALSE, sep = '\t')
        } else {
            w1 <- NULL
        }
        f  <- read.table(paste0('dataset_F', sp, '.', tag, '.tsv'), header = FALSE, sep = '\t')
        t  <- read.table(paste0('dataset_T', sp, '.', tag, '.tsv'), header = FALSE, sep = '\t')
        m  <- matrix(0, ncol = 3, nrow = nrow(f))
        colnames(m) <- c('W1', 'F', 'T')
        rownames(m) <- f[, 1]
        m[w1[, 1], 1] <- w1[, 2]
        m[f[, 1],  2] <- f[, 2]
        m[t[, 1],  3] <- t[, 2]
        write.table(m, paste0('dataset_merged', sp, '.', tag,'.tsv'), quote = FALSE,
                col.names = TRUE, sep = '\t', row.names = TRUE)
    }
}
source('../../scripts/japan_map.R')
```



## Occurence records

The original dataset was obtained from KankyoSho.
The original dataset was preprocessed by the following steps.

```bash
cd ${DATA_PATH}
cd secure_data

cut -d',' -f1,4 Kankyosoh5thTombo.csv | sort | uniq | grep -v mesh2 > kankyosho_public.tsv


python ${SCRIPT_PATH}/generate_meshdataset.py species \
            kankyosho_public.tsv \
            Sp.table.revised.csv \
            ../dragonfly_classes.txt \
            ../meshmatrix.tsv.gz

python ${SCRIPT_PATH}/generate_meshdataset.py genus \
            kankyosho_public.tsv \
            Sp.table.revised.csv \
            ../dragonflyg_classes.txt \
            ../meshmatrixg.tsv.gz
cd ..
mv meshmatrix*summary.tsv data_summary/
```



